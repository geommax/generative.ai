{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdab23bf",
   "metadata": {},
   "source": [
    "## Decoder models learn tasks by prompting.\n",
    "- Decoder-only models are trained to predict the next token given all previous tokens:\n",
    "- They never see explicit “task labels”; everything is learned as a continuation problem.\n",
    "- Trained on billions of tokens from natural text, code, dialogue, etc.\n",
    "- Model learns patterns, reasoning, and associations from language itself.\n",
    "No built-in task head - Unlike BERT (encoder + classifier), GPT has no classifier layer; it just predicts tokens. So any “task” (sentiment, QA, translation) must be encoded in the input sequence.\n",
    "\n",
    "#### The model predicts token by token\n",
    "- Uses attention over the entire prompt (demonstrations + query)\n",
    "- Produces the task answer as a natural continuation\n",
    "\n",
    "#### Encoder models learn tasks by training,"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
