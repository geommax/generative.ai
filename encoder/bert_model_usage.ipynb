{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f2d6e51",
   "metadata": {},
   "source": [
    "## Encoder models learn tasks by training,\n",
    "BERT မှာ “few-shot” ဆိုရင်\n",
    "training data အနည်းငယ်နဲ့ fine-tune လုပ်ပြီး inference လုပ်တာ ကို ဆိုလိုပါတယ်။ BERT ရဲ့ output က [CLS] embedding classifier head က logits ထုတ် ပေးတာဖြစ်ပါတယ်။ label mapping (positive / negative) က BERT pretrained weight ထဲမှာ မရှိပါဘူး။ အဲ့ဒါကို classifier head (sometimes) upper encoder layers ထဲမှာ training လုပ်ပြီး inference လုပ်တယ်လို့ခေါ်ပါတယ်။ \n",
    "- ကိုယ်လိုချင်တဲ့ scope ကို Train စရာမလိုပဲ Classification တန်းလုပ်လို့ရတဲ့ encoder model လည်းရှိပါတယ်။\n",
    "\n",
    "\n",
    "#### Decoder models learn tasks by prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28173910",
   "metadata": {},
   "source": [
    "## Testing Few Shot Classification by training encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c725a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = positive, 0 = negative\n",
    "few_shot_data = [\n",
    "    (\"I love this product, it works perfectly!\", 0),\n",
    "    (\"Absolutely fantastic experience\", 0),\n",
    "    (\"Very happy with the results\", 0),\n",
    "    (\"This is amazing\", 0),\n",
    "    (\"Highly recommended\", 0),\n",
    "\n",
    "    (\"I hate this thing\", 1),\n",
    "    (\"Terrible and disappointing\", 1),\n",
    "    (\"Very bad experience\", 1),\n",
    "    (\"Not worth the money\", 1),\n",
    "    (\"Completely useless\", 1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c188e83",
   "metadata": {},
   "source": [
    "## Init Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0a54671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts: ['I love this product, it works perfectly!', 'Absolutely fantastic experience', 'Very happy with the results', 'This is amazing', 'Highly recommended', 'I hate this thing', 'Terrible and disappointing', 'Very bad experience', 'Not worth the money', 'Completely useless']\n",
      "Labels: tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "texts = [x[0] for x in few_shot_data]\n",
    "labels = torch.tensor([x[1] for x in few_shot_data])\n",
    "\n",
    "print(\"Texts:\", texts)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "encodings = tokenizer(\n",
    "    texts,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff7791e",
   "metadata": {},
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00d15ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=2,\n",
    "    output_attentions=True,\n",
    "    output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d4bfa",
   "metadata": {},
   "source": [
    "### ALL layers are trained by default\n",
    "\n",
    "- Embedding layer\n",
    "- All Transformer encoder layers (12 layers in bert-base)\n",
    "- Final classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91218df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freeze all BERT layers\n",
    "for param in model.parameters(): ## parameters တစ်ခုထဲမလို့ .parameters() သုံးလို့၇တယ်။\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a3e35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder fine-tuning: last 2 layers ကိုပဲ train လုပ်မယ်\n",
    "## classifier layer ကိုလည်း train လုပ်မယ်\n",
    "## Total 4 layers ကိုပဲ train လုပ်မယ်\n",
    "for name, param in model.named_parameters(): ## parameters အများကြီး update လုပ်ဖို့ named_parameters() သုံးရတယ်။\n",
    "    if \"encoder.layer.10\" in name or \"encoder.layer.11\" in name:\n",
    "        param.requires_grad = True\n",
    "    elif \"classifier\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3f764",
   "metadata": {},
   "source": [
    "#### Train All parameters in BERT model (Model ထဲက parameters အားလုံးကို loss တွက်ပြီး train ကြမယ်။)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ee1e839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.6218\n",
      "Epoch 2 | Loss: 0.6950\n",
      "Epoch 3 | Loss: 0.7131\n",
      "Epoch 4 | Loss: 0.6973\n",
      "Epoch 5 | Loss: 0.6779\n",
      "Epoch 6 | Loss: 0.6832\n",
      "Epoch 7 | Loss: 0.6789\n",
      "Epoch 8 | Loss: 0.6860\n",
      "Epoch 9 | Loss: 0.6138\n",
      "Epoch 10 | Loss: 0.7250\n",
      "Epoch 11 | Loss: 0.6802\n",
      "Epoch 12 | Loss: 0.6760\n",
      "Epoch 13 | Loss: 0.7155\n",
      "Epoch 14 | Loss: 0.6353\n",
      "Epoch 15 | Loss: 0.6870\n",
      "Epoch 16 | Loss: 0.6783\n",
      "Epoch 17 | Loss: 0.6648\n",
      "Epoch 18 | Loss: 0.6625\n",
      "Epoch 19 | Loss: 0.6798\n",
      "Epoch 20 | Loss: 0.6890\n",
      "Epoch 21 | Loss: 0.6500\n",
      "Epoch 22 | Loss: 0.6686\n",
      "Epoch 23 | Loss: 0.6433\n",
      "Epoch 24 | Loss: 0.7058\n",
      "Epoch 25 | Loss: 0.6551\n",
      "Epoch 26 | Loss: 0.6890\n",
      "Epoch 27 | Loss: 0.6534\n",
      "Epoch 28 | Loss: 0.7263\n",
      "Epoch 29 | Loss: 0.7130\n",
      "Epoch 30 | Loss: 0.6147\n"
     ]
    }
   ],
   "source": [
    "## Train All parameters in BERT model\n",
    "#for param in model.parameters():\n",
    "#    param.requires_grad = True\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train() # Set the model to training mode\n",
    "\n",
    "for epoch in range(30):  # few epochs only\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=encodings[\"input_ids\"],\n",
    "        attention_mask=encodings[\"attention_mask\"],\n",
    "        labels=labels\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a1fcc",
   "metadata": {},
   "source": [
    "## Evaluate After Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "211fb452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([3, 9])\n",
      "input_ids size: tensor([[ 101, 1045, 7078, 2293, 2023,  999,  102,    0,    0],\n",
      "        [ 101, 2025, 2204, 2012, 2035, 1012,  102,    0,    0],\n",
      "        [ 101, 4151, 4485, 1010, 1045, 5223, 2023, 1012,  102]])\n",
      "attention_mask: torch.Size([3, 9])\n",
      "attention_mask size: tensor([[1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Number of hidden states: 13\n",
      "Embedding output shape: torch.Size([3, 9, 768])\n",
      "Embedding output: tensor([[[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
      "           3.8253e-02,  1.6400e-01],\n",
      "         [-3.4026e-04,  5.3974e-01, -2.8805e-01,  ...,  7.5731e-01,\n",
      "           8.9008e-01,  1.6575e-01],\n",
      "         [ 4.6552e-01,  2.5250e-01, -2.7314e-01,  ...,  5.4588e-01,\n",
      "           4.2764e-01,  6.0232e-01],\n",
      "         ...,\n",
      "         [-1.4815e-01, -2.9485e-01, -1.6900e-01,  ..., -5.0090e-01,\n",
      "           2.5442e-01, -7.0021e-02],\n",
      "         [ 2.4668e-01, -8.4544e-01, -1.1325e-01,  ...,  2.6931e-04,\n",
      "          -1.1196e-02,  1.5729e-01],\n",
      "         [ 2.7045e-01, -8.1214e-01, -2.3954e-01,  ...,  1.5099e-01,\n",
      "          -1.1404e-01,  1.9319e-01]],\n",
      "\n",
      "        [[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
      "           3.8253e-02,  1.6400e-01],\n",
      "         [-1.0533e-01,  4.7686e-01,  2.3135e-01,  ...,  5.7201e-01,\n",
      "          -3.3449e-02,  2.0032e-01],\n",
      "         [ 8.6978e-02,  2.4539e-01, -4.6225e-01,  ..., -7.0246e-01,\n",
      "          -7.8714e-01, -1.5226e+00],\n",
      "         ...,\n",
      "         [-1.4815e-01, -2.9485e-01, -1.6900e-01,  ..., -5.0090e-01,\n",
      "           2.5442e-01, -7.0021e-02],\n",
      "         [ 2.4668e-01, -8.4544e-01, -1.1325e-01,  ...,  2.6931e-04,\n",
      "          -1.1196e-02,  1.5729e-01],\n",
      "         [ 2.7045e-01, -8.1214e-01, -2.3954e-01,  ...,  1.5099e-01,\n",
      "          -1.1404e-01,  1.9319e-01]],\n",
      "\n",
      "        [[ 1.6855e-01, -2.8577e-01, -3.2613e-01,  ..., -2.7571e-02,\n",
      "           3.8253e-02,  1.6400e-01],\n",
      "         [ 7.5559e-01,  7.1250e-01,  5.6219e-01,  ..., -3.4859e-01,\n",
      "          -1.2706e-01, -8.9999e-02],\n",
      "         [ 1.6107e+00, -5.6907e-01,  8.9826e-01,  ...,  4.6473e-01,\n",
      "          -2.2682e-01, -1.5721e-01],\n",
      "         ...,\n",
      "         [-6.6389e-01,  3.5061e-01, -1.2796e-01,  ...,  3.5926e-02,\n",
      "           3.7266e-01,  3.5053e-01],\n",
      "         [-1.7839e-01,  4.5461e-02, -5.2727e-02,  ...,  3.0492e-01,\n",
      "           7.3745e-01,  5.6209e-01],\n",
      "         [-2.3061e-01, -2.4164e-01, -4.0893e-02,  ..., -2.8555e-01,\n",
      "           2.4370e-01, -7.0539e-02]]])\n"
     ]
    }
   ],
   "source": [
    "model.eval() # evaluation mode\n",
    "\n",
    "test_sentences = [\n",
    "    \"I absolutely love this!\",\n",
    "    \"Not good at all.\",\n",
    "    \"Holy Shit, I hate this.\",\n",
    "]\n",
    "\n",
    "test_enc = tokenizer(\n",
    "    test_sentences,\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(\"input_ids:\", test_enc[\"input_ids\"].shape)\n",
    "print(\"input_ids size:\", test_enc[\"input_ids\"])\n",
    "print(\"attention_mask:\", test_enc[\"attention_mask\"].shape)\n",
    "print(\"attention_mask size:\", test_enc[\"attention_mask\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_enc)\n",
    "    predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "hidden_states = outputs.hidden_states\n",
    "attentions = outputs.attentions\n",
    "\n",
    "# hidden state ဆိုတာက Token Embedding နဲ့ → LayerNorm ကြားထဲက + Position Embedding နဲ့ + Segment Embedding matrix တွေကိုပြောတာပါ။\n",
    "\n",
    "print(\"Number of hidden states:\", len(hidden_states))\n",
    "print(\"Embedding output shape:\", hidden_states[0].shape)\n",
    "print(\"Embedding output:\", hidden_states[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77d63756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I absolutely love this! → Negative\n",
      "Not good at all. → Negative\n",
      "Holy Shit, I hate this. → Negative\n"
     ]
    }
   ],
   "source": [
    "for text, pred in zip(test_sentences, predictions):\n",
    "    print(text, \"→\", \"Positive\" if pred.item() == 1 else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fe2f645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder layer 1 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 2 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 3 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 4 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 5 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 6 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 7 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 8 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 9 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 10 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 11 output: torch.Size([3, 9, 768])\n",
      "Encoder layer 12 output: torch.Size([3, 9, 768])\n"
     ]
    }
   ],
   "source": [
    "for i, layer_hidden in enumerate(hidden_states[1:], start=1):\n",
    "    print(f\"Encoder layer {i} output:\", layer_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6033e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS embedding shape: torch.Size([3, 768])\n",
      "CLS embedding: tensor([[ 0.0456,  0.3026,  0.1679,  ..., -0.4577,  0.2394,  0.2425],\n",
      "        [-0.2764,  0.0137, -0.2456,  ..., -0.1348,  0.3168,  0.3745],\n",
      "        [-0.1571,  0.1834, -0.1916,  ..., -0.0787,  0.3466,  0.2784]])\n"
     ]
    }
   ],
   "source": [
    "last_hidden = hidden_states[-1]\n",
    "\n",
    "cls_embedding = last_hidden[:, 0, :]\n",
    "print(\"CLS embedding shape:\", cls_embedding.shape)\n",
    "print(\"CLS embedding:\", cls_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "070ae670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits (manual): torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "classifier = model.classifier\n",
    "\n",
    "logits_manual = classifier(cls_embedding)\n",
    "\n",
    "print(\"Logits (manual):\", logits_manual.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d4e1e9",
   "metadata": {},
   "source": [
    "### Output Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1465d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits (model): torch.Size([3, 2])\n",
      "logits : tensor([[ 0.0982, -0.2283],\n",
      "        [ 0.1364, -0.0842],\n",
      "        [-0.0203, -0.2121]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Logits (model):\", outputs.logits.shape)\n",
    "print(\"logits :\", outputs.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
